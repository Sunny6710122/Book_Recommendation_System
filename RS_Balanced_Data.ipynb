{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Trained with Balanced Data and Rating Scale (1-6)**"
      ],
      "metadata": {
        "id": "odVATthMo4eB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKXaIcvzw7cj",
        "outputId": "29e48c5a-2a7f-40a3-ad13-a6826404d0f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution Before Balancing:\n",
            "Recommendation\n",
            "0    740958\n",
            "1    290217\n",
            "Name: count, dtype: int64\n",
            "Class Distribution After Balancing:\n",
            "Recommendation\n",
            "0    290217\n",
            "1    290217\n",
            "Name: count, dtype: int64\n",
            "Epoch 1/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 51ms/step - accuracy: 0.6619 - loss: 0.5980 - val_accuracy: 0.6996 - val_loss: 0.5631\n",
            "Epoch 2/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 51ms/step - accuracy: 0.7774 - loss: 0.4635 - val_accuracy: 0.6745 - val_loss: 0.6148\n",
            "Epoch 3/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m725s\u001b[0m 50ms/step - accuracy: 0.8262 - loss: 0.3833 - val_accuracy: 0.6698 - val_loss: 0.6414\n",
            "Epoch 4/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m783s\u001b[0m 53ms/step - accuracy: 0.8529 - loss: 0.3226 - val_accuracy: 0.6724 - val_loss: 0.7077\n",
            "Epoch 5/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 50ms/step - accuracy: 0.8727 - loss: 0.2765 - val_accuracy: 0.6609 - val_loss: 0.7629\n",
            "Epoch 6/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m732s\u001b[0m 49ms/step - accuracy: 0.8865 - loss: 0.2438 - val_accuracy: 0.6609 - val_loss: 0.8726\n",
            "Epoch 7/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 49ms/step - accuracy: 0.8996 - loss: 0.2144 - val_accuracy: 0.6517 - val_loss: 0.9004\n",
            "Epoch 8/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m755s\u001b[0m 50ms/step - accuracy: 0.9105 - loss: 0.1908 - val_accuracy: 0.6542 - val_loss: 0.9805\n",
            "Epoch 9/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 51ms/step - accuracy: 0.9193 - loss: 0.1717 - val_accuracy: 0.6435 - val_loss: 1.0874\n",
            "Epoch 10/10\n",
            "\u001b[1m14511/14511\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 51ms/step - accuracy: 0.9287 - loss: 0.1538 - val_accuracy: 0.6433 - val_loss: 1.1525\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6314683dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1. Load Dataset\n",
        "users = pd.read_csv(\"BX-Users.csv\", sep=';', encoding='latin-1', on_bad_lines='skip')\n",
        "books = pd.read_csv(\"BX_Books.csv\", sep=';', encoding='latin-1', on_bad_lines='skip')\n",
        "ratings = pd.read_csv(\"BX-Book-Ratings.csv\", sep=';', encoding='latin-1', on_bad_lines='skip')\n",
        "\n",
        "# Merge ratings with user and book features\n",
        "data = ratings.merge(users, on=\"User-ID\").merge(books, on=\"ISBN\")\n",
        "\n",
        "# Drop unnecessary features\n",
        "data = data.drop(columns=[\"ISBN\", \"Publisher\"])\n",
        "\n",
        "# 3. Handle Missing Values\n",
        "data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].median())\n",
        "data[\"Location\"] = data[\"Location\"].fillna(\"Unknown\")\n",
        "data[\"Book-Title\"] = data[\"Book-Title\"].fillna(\"Unknown\")\n",
        "data[\"Book-Author\"] = data[\"Book-Author\"].fillna(\"Unknown\")\n",
        "data[\"Year-Of-Publication\"] = data[\"Year-Of-Publication\"].fillna(data[\"Year-Of-Publication\"].median())\n",
        "\n",
        "# 4. Encode Features\n",
        "le_user = LabelEncoder()\n",
        "le_book = LabelEncoder()\n",
        "\n",
        "data[\"User-ID\"] = le_user.fit_transform(data[\"User-ID\"])\n",
        "data[\"Book-ID\"] = le_book.fit_transform(data[\"Book-Title\"]) + 1  # Ensure IDs start from 1\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "data[\"Age\"] = scaler.fit_transform(data[\"Age\"].values.reshape(-1, 1))\n",
        "data[\"Year-Of-Publication\"] = scaler.fit_transform(data[\"Year-Of-Publication\"].values.reshape(-1, 1))\n",
        "\n",
        "# Label ratings as 0 or 1\n",
        "data[\"Recommendation\"] = (data[\"Book-Rating\"] > 6).astype(int)\n",
        "\n",
        "# 5. Balance the Dataset\n",
        "class_counts = data[\"Recommendation\"].value_counts()\n",
        "print(\"Class Distribution Before Balancing:\")\n",
        "print(class_counts)\n",
        "\n",
        "positive_class = data[data[\"Recommendation\"] == 1]\n",
        "negative_class = data[data[\"Recommendation\"] == 0]\n",
        "\n",
        "# Downsample the negative class to match the positive class size\n",
        "negative_class_downsampled = negative_class.sample(n=len(positive_class), random_state=42)\n",
        "\n",
        "# Combine the balanced classes\n",
        "balanced_data = pd.concat([positive_class, negative_class_downsampled])\n",
        "\n",
        "# Shuffle the dataset\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Class Distribution After Balancing:\")\n",
        "print(balanced_data[\"Recommendation\"].value_counts())\n",
        "\n",
        "# 6. Split Data (Without Age)\n",
        "X = balanced_data[[\"User-ID\", \"Location\", \"Book-ID\", \"Book-Title\", \"Book-Author\", \"Year-Of-Publication\"]]\n",
        "y = balanced_data[\"Recommendation\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 7. Preprocess Text Features\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=5000, output_sequence_length=10)\n",
        "\n",
        "text_vectorizer.adapt(X_train[\"Location\"])\n",
        "location_train = text_vectorizer(X_train[\"Location\"])\n",
        "location_test = text_vectorizer(X_test[\"Location\"])\n",
        "\n",
        "title_vectorizer = tf.keras.layers.TextVectorization(max_tokens=5000, output_sequence_length=10)\n",
        "title_vectorizer.adapt(X_train[\"Book-Title\"])\n",
        "title_train = title_vectorizer(X_train[\"Book-Title\"])\n",
        "title_test = title_vectorizer(X_test[\"Book-Title\"])\n",
        "\n",
        "author_vectorizer = tf.keras.layers.TextVectorization(max_tokens=5000, output_sequence_length=10)\n",
        "author_vectorizer.adapt(X_train[\"Book-Author\"])\n",
        "author_train = author_vectorizer(X_train[\"Book-Author\"])\n",
        "author_test = author_vectorizer(X_test[\"Book-Author\"])\n",
        "\n",
        "# 8. Define Model (Without Age)\n",
        "user_input = tf.keras.layers.Input(shape=(1,), name=\"User-ID\")\n",
        "location_input = tf.keras.layers.Input(shape=(10,), name=\"Location\")\n",
        "book_input = tf.keras.layers.Input(shape=(1,), name=\"Book-ID\")\n",
        "title_input = tf.keras.layers.Input(shape=(10,), name=\"Book-Title\")\n",
        "author_input = tf.keras.layers.Input(shape=(10,), name=\"Book-Author\")\n",
        "year_input = tf.keras.layers.Input(shape=(1,), name=\"Year-Of-Publication\")\n",
        "\n",
        "user_embedding = tf.keras.layers.Embedding(input_dim=data[\"User-ID\"].nunique(), output_dim=16)(user_input)\n",
        "book_embedding = tf.keras.layers.Embedding(input_dim=data[\"Book-ID\"].nunique() + 1, output_dim=16)(book_input)\n",
        "\n",
        "user_flatten = tf.keras.layers.Flatten()(user_embedding)\n",
        "book_flatten = tf.keras.layers.Flatten()(book_embedding)\n",
        "\n",
        "location_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=8)(location_input)\n",
        "title_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=8)(title_input)\n",
        "author_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=8)(author_input)\n",
        "\n",
        "location_flatten = tf.keras.layers.Flatten()(location_embedding)\n",
        "title_flatten = tf.keras.layers.Flatten()(title_embedding)\n",
        "author_flatten = tf.keras.layers.Flatten()(author_embedding)\n",
        "\n",
        "concat = tf.keras.layers.Concatenate()(\n",
        "    [user_flatten, location_flatten, book_flatten, title_flatten, author_flatten, year_input]\n",
        ")\n",
        "\n",
        "dense1 = tf.keras.layers.Dense(128, activation=\"relu\")(concat)\n",
        "dense2 = tf.keras.layers.Dense(64, activation=\"relu\")(dense1)\n",
        "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense2)\n",
        "\n",
        "model = tf.keras.Model(\n",
        "    inputs=[user_input, location_input, book_input, title_input, author_input, year_input], outputs=output\n",
        ")\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# 9. Prepare Data for Model (Without Age)\n",
        "X_train_prepared = [\n",
        "    X_train[\"User-ID\"],\n",
        "    location_train,\n",
        "    X_train[\"Book-ID\"],\n",
        "    title_train,\n",
        "    author_train,\n",
        "    X_train[\"Year-Of-Publication\"].values,\n",
        "]\n",
        "\n",
        "X_test_prepared = [\n",
        "    X_test[\"User-ID\"],\n",
        "    location_test,\n",
        "    X_test[\"Book-ID\"],\n",
        "    title_test,\n",
        "    author_test,\n",
        "    X_test[\"Year-Of-Publication\"].values,\n",
        "]\n",
        "\n",
        "# 10. Train Model\n",
        "model.fit(\n",
        "    X_train_prepared,\n",
        "    y_train,\n",
        "    validation_data=(X_test_prepared, y_test),\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 10. Evaluate Model\n",
        "loss, accuracy = model.evaluate(X_test_prepared, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p3fRKOovYzy",
        "outputId": "e7fd3f1f-7e02-43a7-9335-e201a10c98d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3628/3628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.6456 - loss: 1.1444\n",
            "Test Accuracy: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, average_precision_score\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions\n",
        "y_pred_probs = model.predict(X_test_prepared)  # Predicted probabilities\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)  # Convert to binary (assuming binary classification)\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_pred, average='binary')  # Use 'micro', 'macro', or 'weighted' for multi-class\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, y_pred, average='binary')\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Mean Average Precision (MAP)\n",
        "average_precision = average_precision_score(y_test, y_pred_probs)\n",
        "print(f\"Mean Average Precision (MAP): {average_precision:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbL-n0yMwOum",
        "outputId": "9ec6624f-8b6e-42d3-ce9b-cd16efce1a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3628/3628\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step\n",
            "Precision: 0.64\n",
            "Recall: 0.66\n",
            "F1 Score: 0.65\n",
            "Confusion Matrix:\n",
            "[[36362 21683]\n",
            " [19720 38322]]\n",
            "Mean Average Precision (MAP): 0.67\n"
          ]
        }
      ]
    }
  ]
}